{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAPILLON Tutorial\n",
    "In this notebook, we will walk through how to set up your own PAPILLON pipeline locally with a GPU server step-by-step.\n",
    "\n",
    "### What is PAPILLON?\n",
    "PrivAcy Preservation from Internet-based and Local Language MOdel ENsembles (PAPILLON) is a framework where trusted but weaker models can use untrusted but more powerful models as tools in order to preserve user inference-time privacy.\n",
    "\n",
    "You can refer to the original paper [here](https://arxiv.org/abs/2410.17127) for how we constructed the benchmark for our task.\n",
    "\n",
    "![Overview of the PAPILLON pipeline](figs/1.png)\n",
    "\n",
    "For this tutorial, we will use **GPT-4o-mini** as the untrusted model and **Llama-3.1-8B-Instruct** as the trusted, locally-hosted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install dspy-ai==2.5.41 openai pandas sglang[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Llama-3.1-8B-Instruct\n",
    "We will host this model using SGLang. If you have the model hosted somewhere else, that should also be okay, you can just adjust the `local_lm` variable accordingly in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/ \n",
    "\n",
    "PORT_NUMBER = 7501 # You can change the port number here\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 python -m sglang.launch_server --port $PORT_NUMBER --model-path meta-llama/Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Local LM and Remote LLM\n",
    "The Local LM would correspond to the trusted (but usually weaker) model. The Local LM should ideally be the only component of the pipeline that manages your private information. The Remote LM would correspond to the untrusted (but usually more potent) model. The goal of the PAPILLON pipeline is to produce high-quality outputs while leaking as little of your private information as possible to the Remote LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "\n",
    "local_lm = dspy.LM('openai/default', api_base=f\"http://127.0.0.1:{PORT_NUMBER}/v1\", api_key=\"\", max_tokens=4000)\n",
    "dspy.configure(lm=local_lm)\n",
    "\n",
    "openai_lm = dspy.LM(model=\"openai/gpt-4o-mini\", max_tokens=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the PAPILLON DSPy Module\n",
    "We will now define the Prompt Creator and Information Aggregator modules according to the diagram earlier in this notebook. \n",
    "\n",
    "After defining the module, we can then optimize the prompts for these modules using the MIPRO v2 DSPy prompt optimizer, so that you can keep creating new PAPILLON pipelines for your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CreateOnePrompt(dspy.Signature):\n",
    "    \"\"\"\n",
    "    You are a helpful assistant that is very mindful of user privacy. You have access to a powerful large language model that you can query. Given a user request, create a prompt for your large language model that preserves user privacy, so that this model can help you complete the user request. Provide the prompt directly without any preamble. DO NOT COMPLETE THE USER QUERY, ONLY GENERATE A PROMPT.\n",
    "    \"\"\"\n",
    "    userQuery = dspy.InputField(desc=\"The user's request to be fulfilled.\")\n",
    "    createdPrompt = dspy.OutputField()\n",
    "\n",
    "class InfoAggregator(dspy.Signature):\n",
    "    \"\"\"\n",
    "    You are a helpful assistant. Respond to queries from the user.\n",
    "    \"\"\"\n",
    "\n",
    "    userQuery = dspy.InputField(desc=\"The user's request to be fulfilled.\")\n",
    "    modelExampleResponses = dspy.InputField(desc=\"Information from a more powerful language model responding to related queries. Complete the user query by referencing this information. Only you have access to this information.\")\n",
    "    finalOutput = dspy.OutputField()\n",
    "\n",
    "\n",
    "class PAPILLON(dspy.Module):\n",
    "    def __init__(self, untrusted_model):\n",
    "        self.prompt_creater = dspy.ChainOfThought(CreateOnePrompt)\n",
    "        self.info_aggregator = dspy.Predict(InfoAggregator)\n",
    "        self.untrusted_model = untrusted_model\n",
    "    \n",
    "    def forward(self, user_query):\n",
    "        try:\n",
    "            prompt = self.prompt_creater(userQuery=user_query).createdPrompt\n",
    "            response = self.untrusted_model(prompt)[0]\n",
    "            output = self.info_aggregator(userQuery=user_query, modelExampleResponses=response)\n",
    "        except Exception:\n",
    "            return dspy.Prediction(prompt=\"\", output=\"\", gptResponse=\"\")\n",
    "        \n",
    "        return dspy.Prediction(prompt=prompt, output=output.finalOutput, gptResponse=response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Your PAPILLON Pipeline\n",
    "#### Let's First Load Some Data\n",
    "\n",
    "In our paper, we proposed the PUPA benchmark, available both on Huggingface (TBD) and in this repository (`pupa/*.csv`).\n",
    "\n",
    "The PUPA benchmark contains user-assistant interactions where the user divulges personally identifiable information (PII) in the [WildChat](https://arxiv.org/abs/2405.01470) dataset. We use annotation schemas detailed in the [Trust No Bot](https://arxiv.org/abs/2407.11438) paper to determine whether an interaction contains private information of specific types. PUPA consists of two parts:\n",
    "1. `PUPA-TNB` is data derived from the original annotations from the Trust No Bot paper.\n",
    "2. `PUPA-New` is data derived from the rest of the WildChat dataset that is not annotated in the Trust No Bot paper.\n",
    "\n",
    "We will use PUPA-New to optimize our PAPILLON program and for evaluation before and after the optimization. In our paper, we used PUPA-TNB for standardized evaluation across different models, but we are just using PUPA-New here for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_query</th>\n",
       "      <th>pii_units</th>\n",
       "      <th>target_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rachel Zheng is a clerk in the Administrative ...</td>\n",
       "      <td>rachel||johnny||univeristy of westminster||wes...</td>\n",
       "      <td>Rachel Zheng 是 H&amp;R Technology 行政部门的职员。当她回到办公室时...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Please describe the job knowledge requirements...</td>\n",
       "      <td>university</td>\n",
       "      <td>The International Advising Assistant position ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i want to write a letter to the royal college ...</td>\n",
       "      <td>Sudan</td>\n",
       "      <td>[Your Name] [Your Address] [City, State, ZIP C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>critically analyze the film You Shall Not Kill...</td>\n",
       "      <td>krzysztof kieślowski</td>\n",
       "      <td>As an AI language model, I am not programmed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>list all job offering from SPARC companies wit...</td>\n",
       "      <td>sparc</td>\n",
       "      <td>I apologize, but I am not able to provide spec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_query  \\\n",
       "0  Rachel Zheng is a clerk in the Administrative ...   \n",
       "1  Please describe the job knowledge requirements...   \n",
       "2  i want to write a letter to the royal college ...   \n",
       "3  critically analyze the film You Shall Not Kill...   \n",
       "4  list all job offering from SPARC companies wit...   \n",
       "\n",
       "                                           pii_units  \\\n",
       "0  rachel||johnny||univeristy of westminster||wes...   \n",
       "1                                         university   \n",
       "2                                              Sudan   \n",
       "3                               krzysztof kieślowski   \n",
       "4                                              sparc   \n",
       "\n",
       "                                     target_response  \n",
       "0  Rachel Zheng 是 H&R Technology 行政部门的职员。当她回到办公室时...  \n",
       "1  The International Advising Assistant position ...  \n",
       "2  [Your Name] [Your Address] [City, State, ZIP C...  \n",
       "3  As an AI language model, I am not programmed t...  \n",
       "4  I apologize, but I am not able to provide spec...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's first take a look at some data examples\n",
    "import pandas\n",
    "from IPython.display import display\n",
    "\n",
    "pupa_tnb = pandas.read_csv(\"pupa/PUPA_TNB.csv\")\n",
    "pupa_tnb = pupa_tnb[['user_query', 'pii_units', 'target_response']].copy()\n",
    "\n",
    "display(pupa_tnb.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above,\n",
    "* `user_query` is the original user query containing private information\n",
    "* `pii_units` are PII information as extracted by GPT-4o-mini; as you can see in the example, there are instances of over-redaction\n",
    "* `target_response` are the original GPT-3.5 or GPT-4 responses according to WildChat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing function\n",
    "from dspy import Example\n",
    "\n",
    "def synthesize_tvt(data_file):\n",
    "    df = pandas.read_csv(data_file, index_col=False)\n",
    "    train, val, test = [], [], []\n",
    "    for i, row in df.iterrows():\n",
    "        if pandas.isna(row[\"pii_units\"]) or not isinstance(row[\"pii_units\"], str) or len(row[\"pii_units\"]) == 0:\n",
    "            continue\n",
    "        new_dp = Example({\"target_response\": row[\"target_response\"],\n",
    "                          \"user_query\": row[\"user_query\"],\n",
    "                          \"pii_str\": row[\"pii_units\"]}).with_inputs(\"user_query\")\n",
    "        if i < 150:\n",
    "            train.append(new_dp)\n",
    "        elif 150 <= i < 300:\n",
    "            val.append(new_dp)\n",
    "        else:\n",
    "            test.append(new_dp)\n",
    "    return train, val, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the optimization process is to:\n",
    "1. Make sure the PAPILLON responses are at least on par with the `target_response`\n",
    "2. Leak as few `pii_units` to the Remote LLM as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the LLM Judge\n",
    "The definition of the LLM judge here is not the same as the definition in our work, for simplicity. If you are interested in the slightly more complex construction, please refer to `papillon/llm_judge.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JudgeQuality(dspy.Signature):\n",
    "    \"\"\"You are comparing the quality of two responses, given a user query. Is response A at least as good as response B? Respond with yes or no.\"\"\"\n",
    "\n",
    "    user_query = dspy.InputField(desc=\"The user's request to be fulfilled.\")\n",
    "    response_A = dspy.InputField()\n",
    "    response_B = dspy.InputField()\n",
    "    output = dspy.OutputField()\n",
    "\n",
    "class JudgeLeakage(dspy.Signature):\n",
    "    \"\"\"Determine whether a piece of information is present in the prompt given. Respond with yes or no.\"\"\"\n",
    "\n",
    "    information_piece = dspy.InputField(desc=\"The piece of information that we wish to check. If this piece of information is a celebrity name, respond with no.\")\n",
    "    prompt = dspy.InputField()\n",
    "    output = dspy.OutputField()\n",
    "\n",
    "class JudgePromptQual(dspy.Signature):\n",
    "    \"\"\"Determine whether a piece of text is a valid prompt for a language model. Respond with yes or no.\"\"\"\n",
    "    prompt_input = dspy.InputField()\n",
    "    output = dspy.OutputField()\n",
    "\n",
    "\n",
    "class LLMJudge(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quality_judge = dspy.ChainOfThought(JudgeQuality)\n",
    "        self.fact_checker = dspy.Predict(JudgeLeakage)\n",
    "        self.prompt_qual = dspy.ChainOfThought(JudgePromptQual)\n",
    "\n",
    "    def forward(self, user_query, og_resp, new_resp=None, updated_query=None, pii_str=None):\n",
    "        if new_resp:\n",
    "            answer = self.quality_judge(user_query=user_query, response_A=new_resp, response_B=og_resp)\n",
    "            judgment = int((answer.output.lower()).startswith(\"yes\"))\n",
    "        else:\n",
    "            judgment = -1\n",
    "        if updated_query:\n",
    "            if isinstance(pii_str, str):\n",
    "                # The pii_str field must be separated by ||\n",
    "                all_pii_pieces = set(pii_str.split(\"||\"))\n",
    "                pii_score = 0\n",
    "                for p in all_pii_pieces:\n",
    "                    answer = self.fact_checker(information_piece=p, prompt=updated_query)\n",
    "                    pii_score += int((answer.output.lower()).startswith(\"yes\"))\n",
    "                pii_score = pii_score / len(all_pii_pieces)\n",
    "            else:\n",
    "                pii_score = -1\n",
    "        else:\n",
    "            pii_score = -1\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            quality=judgment,\n",
    "            leakage=pii_score\n",
    "        )\n",
    "\n",
    "\n",
    "llm_judge = LLMJudge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Metric\n",
    "This will guide the prompt optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(gold, pred, trace=None):\n",
    "    og_model_output, og_user_query, og_pii = gold.target_response, gold.user_query, gold.pii_str\n",
    "    pred_prompt, pred_out = pred.prompt, pred.output\n",
    "    if len(pred_prompt) == 0:\n",
    "        return 0\n",
    "    with dspy.context(lm=openai_lm):\n",
    "        score_dict = llm_judge(user_query=og_user_query, new_resp=pred_out, og_resp=og_model_output,\n",
    "                                            updated_query=pred_prompt, pii_str=og_pii)       \n",
    "        final_quality_score = score_dict.quality\n",
    "        leakage_sc = score_dict.leakage\n",
    "        try:\n",
    "            assert leakage_sc != -1\n",
    "        except AssertionError:\n",
    "            return 0\n",
    "    # Want to maximize quality and minimize percentage of leakage\n",
    "    final_total_score = final_quality_score - leakage_sc\n",
    "    if trace is not None: return final_total_score >= 1\n",
    "    return final_total_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Zeroshot PAPILLON\n",
    "For this section, we will first synthesize the train, validation, and test data from the `PUPA-New` split of PUPA. We will then evaluate the performance of the zero-shot PAPILLON module using the metrics and LLM judge we just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "from dspy.teleprompt import MIPROv2\n",
    "import json\n",
    "\n",
    "DATA_PATH = \"pupa/PUPA_New.csv\"\n",
    "\n",
    "train, val, test = synthesize_tvt(DATA_PATH)\n",
    "zeroshot = PAPILLON(openai_lm)\n",
    "INCOMPLIANCE = 0\n",
    "evaluate = Evaluate(metric=metric, devset=val, num_threads=8, display_progress=True, display_table=5, max_errors=100)\n",
    "try:\n",
    "    eval_score = evaluate(zeroshot)\n",
    "except Exception as e:\n",
    "    INCOMPLIANCE += 1\n",
    "eval_scores = {}\n",
    "eval_scores.update({\"before_optimization\": eval_score})\n",
    "print(eval_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize with MIPRO v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where you want to store the optimized prompts\n",
    "PROMPT_OUTPUT_FILE = \"output_prompt.json\" \n",
    "# You can choose whatever prompt model you would like, we are just sticking with GPT-4o-mini since it is the cheapest\n",
    "# It is important that your task_model is your trusted model (local_lm)\n",
    "teleprompter = MIPROv2(prompt_model=openai_lm, task_model=local_lm, metric=metric, num_candidates=10, init_temperature=1.0)\n",
    "kwargs = dict(num_threads=8, display_progress=True, display_table=0)\n",
    "compiled_prompt_opt = teleprompter.compile(zeroshot, trainset=train, num_batches=200, max_bootstrapped_demos=0, max_labeled_demos=0, eval_kwargs=kwargs)\n",
    "compiled_prompt_opt.save(PROMPT_OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    eval_score = evaluate(compiled_prompt_opt, devset=val, **kwargs)\n",
    "    print(eval_score)\n",
    "    eval_scores.update({\"after_optimization\": eval_score})\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    local_lm.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_FILE = PROMPT_OUTPUT_FILE.replace(\".json\", \"_eval_socres.json\")\n",
    "json.dump(eval_scores, open(EVAL_FILE, \"w+\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Your Optimized PAPILLON Module\n",
    "\n",
    "You have finished optimizing your PAPILLON module! Huzzah!! Now you can just load the newly optimized prompt and use it on user queries similar to those in PUPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priv_prompt = PAPILLON(openai_lm)\n",
    "    \n",
    "priv_prompt.load(PROMPT_OUTPUT_FILE)\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"Your Query > \")\n",
    "    pred = priv_prompt(user_query)\n",
    "    print(\"PAPILLON PROMPT > \", pred.prompt)\n",
    "    print(\"PAPILLON OUTPUT > \", pred.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "papillon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
